{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing by NLTK\n",
    "\n",
    "In this notebook, we will use `NLTK` to preprocess text documents. `NLTK` is a widely used library for Natural Language Processing. In addition to many built-in capabilities, it has interfaces to many corpus, other libraries, and a good online textbook/cookbook (http://www.nltk.org/book/). \n",
    "\n",
    "## Installing NLTK\n",
    "\n",
    "You need to\n",
    "\n",
    "* install NLTK: http://www.nltk.org/install.html\n",
    "* install NLTK data: http://www.nltk.org/data.html\n",
    "\n",
    "If everything is installed correctly, you should be able to run the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Monty Python and the Holy Grail>\n"
     ]
    }
   ],
   "source": [
    "doc = text6\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 10 matches:\n",
      "ell , this is a temperate zone . ARTHUR : The swallow may fly south with the sun or the house marti\n",
      "hey could be carried . SOLDIER # 1 : What ? A swallow carrying a coconut ? ARTHUR : It could grip i\n",
      "In order to maintain air - speed velocity , a swallow needs to beat its wings forty - three times e\n",
      "LDIER # 2 : It could be carried by an African swallow ! SOLDIER # 1 : Oh , yeah , an African swallo\n",
      "wallow ! SOLDIER # 1 : Oh , yeah , an African swallow maybe , but not a European swallow . That ' s\n",
      "an African swallow maybe , but not a European swallow . That ' s my point . SOLDIER # 2 : Oh , yeah\n",
      "ing Arthur and Sir Bedevere , not more than a swallow ' s flight away , had discovered something . \n",
      "scovered something . Oh , that ' s an unladen swallow ' s flight , obviously . I mean , they were m\n",
      "hat is the air - speed velocity of an unladen swallow ? ARTHUR : What do you mean ? An African or E\n",
      "R : What do you mean ? An African or European swallow ? BRIDGEKEEPER : Huh ? I -- I don ' t know th\n"
     ]
    }
   ],
   "source": [
    "word = 'swallow'\n",
    "doc.concordance(word, width=100, lines=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 6 of 6 matches:\n",
      "NNIS : Man ! ARTHUR : Man . Sorry . What knight live in that castle over there ? DENNIS : I ' m thir\n",
      "ste . Who lives in that castle ? WOMAN : No one live there . ARTHUR : Then who is your lord ? WOMAN \n",
      "hee ha ! Ha ha ha ha ... ARTHUR : Where does he live ? OLD MAN : ... Heh heh heh heh ... ARTHUR : Ol\n",
      "eh heh heh ... ARTHUR : Old man , where does he live ? OLD MAN : ... Hee ha ha ha . He knows of a ca\n",
      "eee - wom ! ARTHUR : Those who hear them seldom live to tell the tale ! HEAD KNIGHT : The Knights Wh\n",
      " ,-- HERBERT : Herbert . FATHER : ' Erbert . We live in a bloody swamp . We need all the land we can\n"
     ]
    }
   ],
   "source": [
    "word = 'live'\n",
    "doc.concordance(word, width=100, lines=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "o cruel that no man yet has fought with it and lived ! Bones of full fifty men lie strewn about its\n"
     ]
    }
   ],
   "source": [
    "word = 'lived'\n",
    "doc.concordance(word, width=100, lines=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 25 matches:\n",
      "I , Arthur , son of Uther Pendragon , from the castle of Camelot . King of the Britons , defeator of\n",
      "RTHUR : Man . Sorry . What knight live in that castle over there ? DENNIS : I ' m thirty - seven . A\n",
      " . I am Arthur , King of the Britons . Who ' s castle is that ? WOMAN : King of the who ? ARTHUR : T\n",
      "ood people . I am in haste . Who lives in that castle ? WOMAN : No one live there . ARTHUR : Then wh\n",
      "se are my Knights of the Round Table . Who ' s castle is this ? FRENCH GUARD : This is the castle of\n",
      " s castle is this ? FRENCH GUARD : This is the castle of my master Guy de Loimbard . ARTHUR : Go and\n",
      "ill not show us the Grail , we shall take your castle by force ! FRENCH GUARD : You don ' t frighten\n",
      " DIRECTOR : Action ! HISTORIAN : Defeat at the castle seems to have utterly disheartened King Arthur\n",
      "T : Welcome gentle Sir Knight . Welcome to the Castle Anthrax . GALAHAD : The Castle Anthrax ? ZOOT \n",
      " Welcome to the Castle Anthrax . GALAHAD : The Castle Anthrax ? ZOOT : Yes . Oh , it ' s not a very \n",
      "nd nineteen - and - a - half , cut off in this castle with no one to protect us . Oooh . It is a lon\n",
      "seek the Grail ! I have seen it , here in this castle ! DINGO : Oh no . Oh , no ! Bad , bad Zoot ! G\n",
      "n , and she must pay the penalty . And here in Castle Anthrax , we have but one punishment for setti\n",
      "swamp . Other kings said I was daft to build a castle on a swamp , but I built it all the same , jus\n",
      " what you ' re gonna get , lad : the strongest castle in these islands . HERBERT : But I don ' t wan\n",
      "nd rescue me . I am in the Tall Tower of Swamp Castle .' At last ! A call ! A cry of distress ! This\n",
      "hen . Shall I , sir ? Yeah SCENE 16 : [ inside castle ] PRINCESS LUCKY and GIRLS : [ giggle giggle g\n",
      "and GIRLS : [ giggle giggle giggle ] [ outside castle ] GUEST : ' Morning ! SENTRY # 1 : ' Morning .\n",
      "ight of King Arthur , sir . FATHER : Very nice castle , Camelot . Uh , very good pig country ... LAU\n",
      " pure of spirit may find the Holy Grail in the Castle of uuggggggh '. ARTHUR : What ? MAYNARD : '...\n",
      "uggggggh '. ARTHUR : What ? MAYNARD : '... the Castle of uuggggggh '. BEDEVERE : What is that ? MAYN\n",
      "inging stops ] [ ethereal music ] ARTHUR : The Castle Aaagh . Our quest is at an end ! God be praise\n",
      " of Camelot , to open the doors of this sacred castle , to which God Himself has guided us ! FRENCH \n",
      "f the Lord , we demand entrance to this sacred castle ! FRENCH GUARD : No chance , English bed - wet\n",
      "you do not open this door , we shall take this castle by force ! [ splat ] In the name of God and th\n"
     ]
    }
   ],
   "source": [
    "word = 'CASTLE' \n",
    "doc.concordance(word, width=100, lines=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your own `text` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "from nltk import word_tokenize # sentence => words\n",
    "from nltk import sent_tokenize # document => sentences\n",
    "\n",
    "#str1 = \"to be or not to BE? That's a question. \"\n",
    "str1 = \"To be or not to BE?\\n That's a question. \"\n",
    "\n",
    "tokens = word_tokenize(str1)\n",
    "doc2 = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the statistics\n",
    "\n",
    "* Document length\n",
    "* Vocalbury size\n",
    "* tf(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens = 16967\n",
      "# of unique tokens = 2166\n"
     ]
    }
   ],
   "source": [
    "print('# of tokens = {}'.format(len(doc)))\n",
    "print('# of unique tokens = {}'.format(len(set(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: To be or not to BE ? That...>\n",
      "# of tokens = 12\n",
      "# of unique tokens = 12\n",
      "[\"'s\", '.', '?', 'BE', 'That', 'To', 'a', 'be', 'not', 'or', 'question', 'to']\n",
      "Displaying 2 of 2 matches:\n",
      "                                              To be or not to BE ? That 's a question .\n",
      "                                 To be or not to BE ? That 's a question .\n"
     ]
    }
   ],
   "source": [
    "print(doc2)\n",
    "print('# of tokens = {}'.format(len(doc2)))\n",
    "print('# of unique tokens = {}'.format(len(set(doc2))))\n",
    "print(sorted(set(doc2)))\n",
    "doc2.concordance('be', width=100, lines=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(doc2.count('to'))\n",
    "print(doc2.count('To'))\n",
    "print(doc2.count('be'))\n",
    "print(doc2.count('BE'))\n",
    "print(doc2.count('bE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "occ = doc2.index('to')\n",
    "print(occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'or', 'not', 'to', 'BE', '?', 'That']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_window = 3\n",
    "doc2[occ - half_window : occ + half_window +1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':', 1197),\n",
       " ('.', 816),\n",
       " ('!', 801),\n",
       " (',', 731),\n",
       " (\"'\", 421),\n",
       " ('[', 319),\n",
       " (']', 312),\n",
       " ('the', 299),\n",
       " ('I', 255),\n",
       " ('ARTHUR', 225),\n",
       " ('?', 207),\n",
       " ('you', 204),\n",
       " ('a', 188),\n",
       " ('of', 158),\n",
       " ('--', 148),\n",
       " ('to', 144),\n",
       " ('s', 141),\n",
       " ('and', 135),\n",
       " ('#', 127),\n",
       " ('...', 118)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = FreqDist(doc)\n",
    "fd.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299, 5, 10]\n"
     ]
    }
   ],
   "source": [
    "words = ['the', 'knight', 'swallow']\n",
    "print( [fd[word] for word in words] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations and n-grams\n",
    "\n",
    "Collocations are good for getting a quick glimpse of what a text is about. `Collocations(num = VAL)` returns multi-word expressions that commonly co-occur. Notice that is not necessarily related to the frequency of the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACK KNIGHT; clop clop; HEAD KNIGHT; mumble mumble; Holy Grail;\n",
      "squeak squeak; FRENCH GUARD; saw saw; Sir Robin; Run away; CARTOON\n",
      "CHARACTER; King Arthur; Iesu domine; Pie Iesu; DEAD PERSON; Round\n",
      "Table; clap clap; OLD MAN; dramatic chord; dona eis; eis requiem; LEFT\n",
      "HEAD; FRENCH GUARDS; music stops; Sir Launcelot; MIDDLE HEAD; RIGHT\n",
      "HEAD; Sir Galahad; angels sing; Arthur music\n"
     ]
    }
   ],
   "source": [
    "doc.collocations(num=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.ngrams(text, n)` returns a *generator* of all n-grams in `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SCENE', '1', ':'), ('1', ':', '['), (':', '[', 'wind'), ('[', 'wind', ']'), ('wind', ']', '['), (']', '[', 'clop'), ('[', 'clop', 'clop'), ('clop', 'clop', 'clop'), ('clop', 'clop', ']'), ('clop', ']', 'KING')]\n"
     ]
    }
   ],
   "source": [
    "print(list(nltk.ngrams(doc, 3))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stopwords and Regular Expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16967\n",
      "2166\n",
      "13288\n",
      "2034\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "print(len(text6))\n",
    "print(len(set(text6)))\n",
    "new_text6 = [w for w in text6 if w not in stopwords]\n",
    "print(len(new_text6))\n",
    "print(len(set(new_text6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['able', 'able', 'absolutely']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "newer_text6 = [w for w in new_text6 if re.search('^ab',w)]\n",
    "print(len(newer_text6))\n",
    "print(newer_text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Lemmatization\n",
    "\n",
    "In the following, we demonstrate \n",
    "* sentence segmentation\n",
    "* tokenizaiton \n",
    "* normalization\n",
    "* stemming\n",
    "* lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to BE ? That 's a question .\n"
     ]
    }
   ],
   "source": [
    "raw = \" \".join(list(doc2))\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To be or not to BE ?', \"That 's a question .\"]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(raw)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To', 'be', 'or', 'not', 'to', 'BE', '?', 'That', \"'s\", 'a', 'question', '.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'be', 'or', 'not', 'to', 'be', '?', 'that', \"'s\", 'a', 'question', '.']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_tokens = [tk.lower() for tk in tokens]\n",
    "lc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To', 'be', 'or', 'not', 'to', 'BE', '?', 'That', \"'s\", 'a', 'question', '.']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "stemmed_tokens = [porter.stem(tk) for tk in tokens]\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpu   linguist   propos   that   reliabl   languag   analysi   is   more   feasibl   with   corpora   collect   in   the   field   ,   in   their   natur   context   ,   and   with   minim   experimental-interfer   .\n"
     ]
    }
   ],
   "source": [
    "raw1 = 'Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field, in their natural contexts, and with minimal experimental-interference.'\n",
    "tokens = nltk.word_tokenize(raw1)\n",
    "print(\"   \".join(stemmed_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tokenization algorithm is pretty smart in not splitting `experimental-interference` into two tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpu   linguist   propos   that   reliabl   languag   analysi   is   more   feasibl   with   corpora   collect   in   the   field   ,   in   their   natur   context   ,   and   with   minim   experimental-interfer   .\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens = [porter.stem(tk) for tk in tokens]\n",
    "print(\"   \".join(stemmed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus   linguistics   proposes   that   reliable   language   analysis   is   more   feasible   with   corpus   collected   in   the   field   ,   in   their   natural   context   ,   and   with   minimal   experimental-interference   .\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized_tokens = [wnl.lemmatize(tk) for tk in tokens]\n",
    "print(\"   \".join(lemmatized_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably wonder by `proposes` above is not lemmatized to `propose`. This is because `lemmatize` method has a default optional parameter `pos = 'n'` (i.e., treating the `proposes` as a noun). If we specify the correct POS tag (`'v'` for verb), the output will be correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'propose'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('proposes', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('is', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Write a function that lemmatizes all words in a sentence by considering their POS tags. \n",
    " \n",
    "Wordnet only accepts the following POS tag: (from the source: http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html)\n",
    "`ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'`\n",
    "\n",
    "You can use `nltk.pos_tag(tokens)` to obtain POS tags for the input token list. \n",
    "\n",
    "Your output should look like this (the tabular output is just showing the additional debugging info)\n",
    "\n",
    "```\n",
    "% proper_lemmatize_sentence(raw1, True)\n",
    "\n",
    "\n",
    "                       token/POS           lemmatized_token\n",
    "0                     Corpus/NNP                     Corpus\n",
    "1                linguistics/NNS                linguistics\n",
    "2                   proposes/VBZ                    propose\n",
    "3                        that/IN                       that\n",
    "4                    reliable/JJ                   reliable\n",
    "5                    language/NN                   language\n",
    "6                    analysis/NN                   analysis\n",
    "7                         is/VBZ                         be\n",
    "8                       more/RBR                       more\n",
    "9                    feasible/JJ                   feasible\n",
    "10                       with/IN                       with\n",
    "11                   corpora/NNS                     corpus\n",
    "12                 collected/VBN                    collect\n",
    "13                         in/IN                         in\n",
    "14                        the/DT                        the\n",
    "15                      field/NN                      field\n",
    "16                           ,/,                          ,\n",
    "17                         in/IN                         in\n",
    "18                    their/PRP$                      their\n",
    "19                    natural/JJ                    natural\n",
    "20                   contexts/NN                    context\n",
    "21                           ,/,                          ,\n",
    "22                        and/CC                        and\n",
    "23                       with/IN                       with\n",
    "24                    minimal/JJ                    minimal\n",
    "25  experimental-interference/NN  experimental-interference\n",
    "26                           ./.                          .\n",
    "\n",
    "['Corpus',\n",
    " 'linguistics',\n",
    " 'propose',\n",
    " 'that',\n",
    " 'reliable',\n",
    " 'language',\n",
    " 'analysis',\n",
    " 'be',\n",
    " 'more',\n",
    " 'feasible',\n",
    " 'with',\n",
    " 'corpus',\n",
    " 'collect',\n",
    " 'in',\n",
    " 'the',\n",
    " 'field',\n",
    " ',',\n",
    " 'in',\n",
    " 'their',\n",
    " 'natural',\n",
    " 'context',\n",
    " ',',\n",
    " 'and',\n",
    " 'with',\n",
    " 'minimal',\n",
    " 'experimental-interference',\n",
    " '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
